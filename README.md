<h1 style="color: #1f8dd6; text-align: center;">My Machine Learning Journey</h1>
<p style="text-align: center;">This repository documents my personal machine learning journey, mostly including the Kaggle competitions I have attended, along with some notes and projects. It serves as a record of my progress and growth in the field of machine learning.</p>

<p style="text-align: center;"><img alt="coding" width="400" src="https://media.giphy.com/media/iPj5oRtJzQGxwzuCKV/giphy.gif"></p>

<h1 style="color: #1f8dd6; text-align: center;">List of Competitions</h1>

<ol>
  <li>
    <h2>House Price Prediction-Advanced Regression</h2>
    <p>
      <strong>Description:</strong> The House Price Advanced Regression competition aimed to predict the sale prices of residential homes based on various features such as area, number of bedrooms, location, etc. The objective was to develop a regression model that could accurately estimate the sale price of a house given its characteristics.
    </p>
    <p>
      <strong>Dataset:</strong> The competition provided a dataset containing a wide range of features for each house, including numerical, categorical, and ordinal variables. The dataset included both training and testing sets, with the target variable being the sale price. Unfortunately, I do not have a specific link to the dataset as it was hosted on Kaggle's competition page.
    </p>
    <p>
      <strong>Approach:</strong> To tackle the problem, I followed a systematic approach. Here is a summary of the key steps I took:
    </p>
    <ol>
      <li>Data Exploration and Preprocessing: I started by exploring the dataset, examining the distribution of variables, identifying missing values, and understanding the relationships between features. I performed necessary data preprocessing steps, such as handling missing values, encoding categorical variables, and addressing outliers.</li>
      <li>Feature Engineering: I performed feature engineering techniques to enhance the predictive power of the model. This involved creating new features, transforming variables, and considering interactions between variables. I also applied log transformations to skewed numerical features to achieve a more normal distribution.</li>
      <li>Model Selection and Training: I experimented with various regression models, including linear regression, decision trees, random forests, and gradient boosting algorithms. I trained each model using cross-validation techniques to evaluate their performance. Hyperparameter tuning was performed to optimize the selected models.</li>
      <li>Model Evaluation and Fine-tuning: I evaluated the models based on common regression metrics such as mean squared error (MSE), root mean squared error (RMSE), and R-squared. I analyzed the performance of different models and iteratively fine-tuned them to achieve the best results.</li>
    </ol>
    <p>
      <strong>Results:</strong> The performance of the models varied, but the best-performing model achieved an RMSE of 0.1, indicating a relatively accurate prediction of house sale prices. Notable findings from the competition included the importance of feature engineering in improving model performance and the effectiveness of ensemble methods for regression tasks.
    </p>
  </li>
</ol>
